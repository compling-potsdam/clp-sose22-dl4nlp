{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "__Advanced NLP | MSc CogSys | Potsdam University__\n",
    "\n",
    "We can install PyTorch following the instructions [here](https://pytorch.org/get-started/locally/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on [this tutorial](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e?#ea0d) and on PyTorch's [documentation](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "\n",
    "**Make sure you always check and double check the documentation when you implement a model with Pytorch!** Even if you have experience with Pytorch, it's super easy to mess up the input and output shapes and meaning of each dimension and end up with a bug that is hard to find later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.nn: implements layers, loss functions and activation functions\n",
    "import torch.nn as nn\n",
    "# torch.optim: implements optimization algorithms (e.g. SGD)\n",
    "import torch.optim as optim\n",
    "# used for activation functions (deprecated?, but still used in PyTorch's github examples)\n",
    "import torch.nn.functional as F \n",
    "# torchviz to visualize computation graphs (needs graphviz)\n",
    "import torchviz\n",
    "# we use numpy for some examples\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the lecture slides, using the Graphics Processing Unit may boost the performance of our program when we are training neural networks. So let us first examine the possibility of using it in our PyTorch code. \n",
    "\n",
    "To use the GPU, PyTorch needs [Cuda](https://pytorch.org/docs/stable/notes/cuda.html), which is a Nvidia's API. In general, PyTorch's installation includes it.\n",
    "\n",
    "We can use ``cuda.is_available()`` to check whether there is any available GPU. If it returns True, we can make our calculations on the GPU. If it returns False, it can either mean that our computer or server has no GPU, or that we missed installing cuda. In that case, all computation will be performed in the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if cuda is available, the GPU will not be automatically used. We have to send our data and models to it ourselves. We will see how to do that, but first it is good practice to store the device we want to use in a variable that will be passed as arguments later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook can use cuda for running the code.\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if it is available, otherwise use CPU.\n",
    "my_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('This notebook can use ' + str(my_device) + ' for running the code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some servers may have more than one GPU. In that case, we can manually choose which one we would like to use. ``torch.cuda.device_count()`` returns the number of available GPUs, which are numbered starting at 0. If there is more than one, we can refer to them using ``\"cuda:0\"``, ``\"cuda:1\"``, ..., ``\"cuda:n\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device(type='cuda', index=0), device(type='cuda', index=1)]\n"
     ]
    }
   ],
   "source": [
    "# a list with all GPUs\n",
    "# torch.device(\"cuda\", n) is the same as torch.device(\"cuda:n\")\n",
    "devices = [torch.device(\"cuda\", n) for n in range(torch.cuda.device_count())]\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remember__: if we want to use the GPU, all our data, parameters and models must be sent to the GPU, preferably by the time they are about to be used. Trying to combine tensors in the CPU with tensors in the GPU in a function will throw an error. GPU RAM is usually much less than CPU RAM, so we need to use it wisely (i.e. we cannot store all data on the GPU at once when we are working with large datasets and models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the fundamental objects in PyTorch. They are n-dimensional arrays, very similar to Numpy arrays, with special methods. \n",
    "\n",
    "You can create a simple tensor like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([1, 2, 3, 4])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "tensor([[[1, 2, 3, 4],\n",
      "         [5, 6, 7, 8]],\n",
      "\n",
      "        [[8, 7, 6, 5],\n",
      "         [4, 3, 2, 1]],\n",
      "\n",
      "        [[0, 2, 4, 6],\n",
      "         [0, 3, 5, 7]]])\n"
     ]
    }
   ],
   "source": [
    "# a scalar\n",
    "tnsr0 = torch.tensor(1)\n",
    "# a vector\n",
    "tnsr1 = torch.tensor([1,2,3,4]) \n",
    "# a matrix\n",
    "tnsr2 = torch.tensor([[1,2,3,4], [5,6,7,8]])\n",
    "# a 3-dimensional tensor\n",
    "tnsr3 = torch.tensor([ [[1,2,3,4], [5,6,7,8]], [[8,7,6,5], [4,3,2,1]], [[0,2,4,6], [0,3,5,7]] ]) \n",
    "\n",
    "print(tnsr0)\n",
    "print(tnsr1)\n",
    "print(tnsr2)\n",
    "print(tnsr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn a Numpy array into a tensor using ``torch.from_numpy()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array([[1,2,3],[4,5,6]])\n",
    "pytorch_tensor = torch.from_numpy(np_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, tensors are stored at the CPU. We can move it to the GPU when we create it by passing the device as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnsr4 = torch.tensor([1,2,3], device=my_device) # my_device can be \"cpu\", \"cuda\" or \"cuda:n\"\n",
    "tnsr4.device # check in which device is the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the device of an existing tensor using ``.to()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnsr2.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, ``.to()`` shadows a tensor's gradient (we will learn that below), so it is advisible to create the tensor directly on the GPU using the argument ``device``, or send it to the GPU before you start using it on your computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a tensor, we can also specify the desired [data type](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype), e.g. ``torch.float``, ``torch.double``, ``torch.long``. If no argument is given, the data type will be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tnsr5 = torch.tensor([1,2,3], device=my_device, dtype=torch.double)\n",
    "print(tnsr5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empty tensors or tensors filled with zeros / random numbers are created by passing the desired dimensions to ``torch.empty()``, ``torch.zeros()`` and ``torch.rand()``, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.8750, 0.0000],\n",
      "        [2.0000, 0.0000, 2.1250],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000]], device='cuda:0')\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0')\n",
      "tensor([[0.3028, 0.7707, 0.3712],\n",
      "        [0.9463, 0.1604, 0.9665],\n",
      "        [0.5573, 0.9001, 0.9463],\n",
      "        [0.2651, 0.2244, 0.4321]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# empty tensor means no values assigned yet, so it is initilized with 'garbage values'\n",
    "empty = torch.empty(4, 3, device=my_device) \n",
    "zeros = torch.zeros(4, 3, device=my_device)\n",
    "rnd = torch.rand(4, 3, device=my_device)\n",
    "print(empty)\n",
    "print(zeros)\n",
    "print(rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of a tensor using ``.shape`` or ``.size()``. In our example, tnsr3 has 3 dimensions. The first has size 3 (number of 'matrices'), the second has size 2 (number of rows) and the third has size 4 (number of columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnsr3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the dimensions of a tensor, we use ``.view()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2]), torch.Size([2, 4]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnsr6 = tnsr2.view(-1, 2) # meaning we want 2 columns, and -1 let it infer the number of needed rows\n",
    "tnsr6.shape, tnsr2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we want to specify only one dimension and let PyTorch infer the remaining dimensions of a tensor, we can use -1 for the unspecified dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to build a new tensor that has the same shape, device and data type of another one, but is filled with zeros or ones, we can use ``zeros_like()`` and ``ones_like()``, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]],\n",
       "\n",
       "        [[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnsr7 = torch.ones_like(tnsr3)\n",
    "tnsr7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access an element in a tensor by its indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnsr3[2,1,3] # [tensor, row, column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``dir(a_tensor)`` returns a description of all methods related to a tensor. Several of them are similar to those in a Numpy array. Some useful ones are:\n",
    "- ``.item()``: returns the value of a tensor as a plain Python number\n",
    "- ``.numpy()``: converts a tensor into a Numpy array (tensor must be on CPU)\n",
    "- ``.device()``: returns tensor location (CPU or GPU)\n",
    "- ``.type()``: returns data type of a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important tip__: it is really easy to get lost with the dimensions of tensors, and the dimensions of inputs and outputs of layers and functions. A good practice is including comments with the number of dimensions, their size and what they represent for each tensor and inputs/outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py) is PyTorch's package of automatic differentiation for operations involving tensors. It [\"records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors\"](https://pytorch.org/docs/stable/notes/autograd.html).\n",
    "\n",
    "Whenever we create a tensor, we can decide if such values require a gradient computation using the argument ``requires_grad``. It is False by default. If set to True, all operations involving such tensor will keep track of the backward operations required to compute its gradient, using the chain rule for derivation. \n",
    "\n",
    "All parameters we want to train in a neural network (weights) should thus be tensors whose argument requires_grad is set to True. Parameters that remain fixed during training and data tensors requires no gradient computation, so we should not waste memory by trying to keep their gradients.\n",
    "\n",
    "Gradients can only be recorded with tensors whose data type is floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(60., device='cuda:0', grad_fn=<DotBackward>), True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1,2,3], device=my_device, requires_grad=True, dtype=torch.float) \n",
    "data = torch.tensor([10,10,10], device=my_device, requires_grad=False, dtype=torch.float)\n",
    "result = params.dot(data)\n",
    "result, result.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we created a tensor of parameters (that requires gradient computation) and a tensor of data. Whenever we use a tensor that requires gradient in a function, the resulting tensor will also require a gradient, besides having an attribute ``.grad_fn`` that explicitly tells which backward operation is needed for PyTorch to calculate the gradient at this step during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, <DotBackward at 0x7f3798f03748>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(params.grad_fn, data.grad_fn, result.grad_fn) # None when there was no function involved on the creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of a tensor is accumulated into its ``.grad`` attribute. Until we perform backpropagation, it is empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/blasota/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad, data.grad, result.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we finish computing our goal result (normally the loss function in a neural network), we call the method ``.backward()`` on the final tensor. This will automatically compute all gradients. Let us see that with a pseudo loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3600., device='cuda:0', grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = result**2\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may look like nothing happened. But we can see the difference on the ``.grad``attributes of all initial tensors that required gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/blasota/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1200., 1200., 1200.], device='cuda:0'), None, None, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad, data.grad, result.grad, loss.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our params tensor has now the value of the gradient for each respective entry. Although result and loss are tensors whose flag requires_grad was set to true, they were simply intermediate steps in the computation of the loss. They serve to keep track of the reverse operations needed for backpropagation, but only the original parameters (whose attributes ``.is_leaf AND .requires_grad`` are True) will have computed gradient values after using the ``.backward()`` method.\n",
    "\n",
    "``.is_leaf`` refers to the tensor being a leaf on the computation graph. Let us see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"109pt\" height=\"271pt\"\n",
       " viewBox=\"0.00 0.00 109.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-267 105,-267 105,4 -4,4\"/>\n",
       "<!-- 139878971973776 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139878971973776</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"77.5,-31 23.5,-31 23.5,0 77.5,0 77.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n",
       "</g>\n",
       "<!-- 139876616698512 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139876616698512</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"95,-86 6,-86 6,-67 95,-67 95,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 139876616698512&#45;&gt;139878971973776 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139876616698512&#45;&gt;139878971973776</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-66.9688C50.5,-60.1289 50.5,-50.5621 50.5,-41.5298\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-41.3678 50.5,-31.3678 47.0001,-41.3678 54.0001,-41.3678\"/>\n",
       "</g>\n",
       "<!-- 139876765808456 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139876765808456</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"92,-141 9,-141 9,-122 92,-122 92,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">DotBackward</text>\n",
       "</g>\n",
       "<!-- 139876765808456&#45;&gt;139876616698512 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139876765808456&#45;&gt;139876616698512</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-121.9197C50.5,-114.9083 50.5,-105.1442 50.5,-96.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-96.3408 50.5,-86.3408 47.0001,-96.3409 54.0001,-96.3408\"/>\n",
       "</g>\n",
       "<!-- 139876616698288 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139876616698288</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139876616698288&#45;&gt;139876765808456 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139876616698288&#45;&gt;139876765808456</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-176.9197C50.5,-169.9083 50.5,-160.1442 50.5,-151.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-151.3408 50.5,-141.3408 47.0001,-151.3409 54.0001,-151.3408\"/>\n",
       "</g>\n",
       "<!-- 139878972134672 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139878972134672</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-263 23.5,-263 23.5,-232 77.5,-232 77.5,-263\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (3)</text>\n",
       "</g>\n",
       "<!-- 139878972134672&#45;&gt;139876616698288 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139878972134672&#45;&gt;139876616698288</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-231.791C50.5,-224.0249 50.5,-214.5706 50.5,-206.3129\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-206.0647 50.5,-196.0648 47.0001,-206.0648 54.0001,-206.0647\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f37900cfac8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the computation graph of a function\n",
    "torchviz.make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blue boxes: parameters of our model, for which we need to compute gradients\n",
    "- Gray boxes: an operation that involves gradient computing tensors\n",
    "- Green box: starting point to compute gradients, if we call ``.backward()`` on this variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us built a more elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"228pt\" height=\"436pt\"\n",
       " viewBox=\"0.00 0.00 228.00 436.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 432)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-432 224,-432 224,4 -4,4\"/>\n",
       "<!-- 139876616778232 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139876616778232</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"136.5,-31 82.5,-31 82.5,0 136.5,0 136.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n",
       "</g>\n",
       "<!-- 139876616757320 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139876616757320</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"154,-86 65,-86 65,-67 154,-67 154,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">SumBackward0</text>\n",
       "</g>\n",
       "<!-- 139876616757320&#45;&gt;139876616778232 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>139876616757320&#45;&gt;139876616778232</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109.5,-66.9688C109.5,-60.1289 109.5,-50.5621 109.5,-41.5298\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"113.0001,-41.3678 109.5,-31.3678 106.0001,-41.3678 113.0001,-41.3678\"/>\n",
       "</g>\n",
       "<!-- 139876616757376 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139876616757376</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"154,-141 65,-141 65,-122 154,-122 154,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 139876616757376&#45;&gt;139876616757320 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139876616757376&#45;&gt;139876616757320</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109.5,-121.9197C109.5,-114.9083 109.5,-105.1442 109.5,-96.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"113.0001,-96.3408 109.5,-86.3408 106.0001,-96.3409 113.0001,-96.3408\"/>\n",
       "</g>\n",
       "<!-- 139876616757880 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139876616757880</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"203,-196 120,-196 120,-177 203,-177 203,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">CosBackward</text>\n",
       "</g>\n",
       "<!-- 139876616757880&#45;&gt;139876616757376 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139876616757880&#45;&gt;139876616757376</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M152.4423,-176.9197C145.0768,-169.1293 134.4983,-157.9405 125.6839,-148.6176\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.2175,-146.2028 118.8041,-141.3408 123.1309,-151.0119 128.2175,-146.2028\"/>\n",
       "</g>\n",
       "<!-- 139876616758048 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139876616758048</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"203,-251 120,-251 120,-232 203,-232 203,-251\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">DotBackward</text>\n",
       "</g>\n",
       "<!-- 139876616758048&#45;&gt;139876616757880 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139876616758048&#45;&gt;139876616757880</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.5,-231.9197C161.5,-224.9083 161.5,-215.1442 161.5,-206.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"165.0001,-206.3408 161.5,-196.3408 158.0001,-206.3409 165.0001,-206.3408\"/>\n",
       "</g>\n",
       "<!-- 139876616758160 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139876616758160</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"102,-306 13,-306 13,-287 102,-287 102,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">PowBackward0</text>\n",
       "</g>\n",
       "<!-- 139876616758160&#45;&gt;139876616758048 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139876616758160&#45;&gt;139876616758048</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M75.6154,-286.9197C91.8455,-278.3365 115.8759,-265.6281 134.3937,-255.8351\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.0395,-258.924 143.2432,-251.155 132.767,-252.7361 136.0395,-258.924\"/>\n",
       "</g>\n",
       "<!-- 139876616757936 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>139876616757936</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"102,-196 13,-196 13,-177 102,-177 102,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"57.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">Log2Backward</text>\n",
       "</g>\n",
       "<!-- 139876616758160&#45;&gt;139876616757936 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139876616758160&#45;&gt;139876616757936</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M57.5,-286.7382C57.5,-268.7541 57.5,-230.0652 57.5,-206.3599\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"61.0001,-206.0828 57.5,-196.0828 54.0001,-206.0829 61.0001,-206.0828\"/>\n",
       "</g>\n",
       "<!-- 139876616758328 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139876616758328</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"101,-361 0,-361 0,-342 101,-342 101,-361\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139876616758328&#45;&gt;139876616758160 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139876616758328&#45;&gt;139876616758160</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M51.7193,-341.9197C52.6117,-334.9083 53.8544,-325.1442 54.959,-316.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.4569,-316.7027 56.2475,-306.3408 51.5129,-315.8189 58.4569,-316.7027\"/>\n",
       "</g>\n",
       "<!-- 139876616777872 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139876616777872</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"77.5,-428 23.5,-428 23.5,-397 77.5,-397 77.5,-428\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (3)</text>\n",
       "</g>\n",
       "<!-- 139876616777872&#45;&gt;139876616758328 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139876616777872&#45;&gt;139876616758328</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M50.5,-396.791C50.5,-389.0249 50.5,-379.5706 50.5,-371.3129\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"54.0001,-371.0647 50.5,-361.0648 47.0001,-371.0648 54.0001,-371.0647\"/>\n",
       "</g>\n",
       "<!-- 139876616758216 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139876616758216</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"204,-306 127,-306 127,-287 204,-287 204,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.5\" y=\"-294\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">MvBackward</text>\n",
       "</g>\n",
       "<!-- 139876616758216&#45;&gt;139876616758048 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139876616758216&#45;&gt;139876616758048</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M164.8033,-286.9197C164.2933,-279.9083 163.5832,-270.1442 162.952,-261.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.4319,-261.0606 162.2157,-251.3408 159.4504,-261.5684 166.4319,-261.0606\"/>\n",
       "</g>\n",
       "<!-- 139876616758384 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139876616758384</title>\n",
       "<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"220,-361 119,-361 119,-342 220,-342 220,-361\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-349\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139876616758384&#45;&gt;139876616758216 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139876616758384&#45;&gt;139876616758216</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M168.8033,-341.9197C168.2933,-334.9083 167.5832,-325.1442 166.952,-316.4652\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"170.4319,-316.0606 166.2157,-306.3408 163.4504,-316.5684 170.4319,-316.0606\"/>\n",
       "</g>\n",
       "<!-- 139876616785712 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139876616785712</title>\n",
       "<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"199,-428 140,-428 140,-397 199,-397 199,-428\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5\" y=\"-404\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> (3, 3)</text>\n",
       "</g>\n",
       "<!-- 139876616785712&#45;&gt;139876616758384 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139876616785712&#45;&gt;139876616758384</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.5,-396.791C169.5,-389.0249 169.5,-379.5706 169.5,-371.3129\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"173.0001,-371.0647 169.5,-361.0648 166.0001,-371.0648 173.0001,-371.0647\"/>\n",
       "</g>\n",
       "<!-- 139876616757936&#45;&gt;139876616757376 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139876616757936&#45;&gt;139876616757376</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M66.5577,-176.9197C73.9232,-169.1293 84.5017,-157.9405 93.3161,-148.6176\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"95.8691,-151.0119 100.1959,-141.3408 90.7825,-146.2028 95.8691,-151.0119\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f3798efefd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1 = torch.tensor([[1,1,1], [2,2,2], [3,3,3]], device=my_device, requires_grad=True, dtype=torch.float)\n",
    "params2 = torch.tensor([0.5, 0.1, 0.9], device=my_device, requires_grad=True, dtype=torch.float)\n",
    "data = torch.tensor([10, 10, 10], device=my_device, requires_grad=False, dtype=torch.float)\n",
    "\n",
    "aux1 = params1.matmul(data)\n",
    "aux2 = params2**0.5\n",
    "aux3 = aux2.dot(aux1)\n",
    "aux4 = torch.cos(aux3) + torch.log2(aux2)\n",
    "\n",
    "loss = torch.sum(aux4)\n",
    "\n",
    "torchviz.make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tensor data is not shown in the computation graph, because it only presents gradient computing tensors and their dependencies. Still, we know that the data tensor was needed for the gray box MvBackward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform backpropagation on this loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.0195, 2.0195, 2.0195],\n",
       "         [0.9032, 0.9032, 0.9032],\n",
       "         [2.7095, 2.7095, 2.7095]], device='cuda:0'),\n",
       " tensor([ 7.5013, 34.3085, 14.3490], device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1.grad, params2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform gradient descent, we need to update the parameters using the newly computed gradient values. We could do it manually by calling ``params1 = params1 - params1.grad``, but it can be really cumbersome when we have many parameters at different steps in a model.\n",
    "\n",
    "PyTorch provides optimizers that can do that automatically and more efficiently. Before learning about them on the next section, there is an important reminder to finish this part.\n",
    "\n",
    "The computation of gradients is cumulative. PyTorch keeps track of operations until we explicitly give a command to disconnet the next computations from the past. For instance, in gradient descent: we output predictions, compute the loss, compute the gradients, update the parameters. An iteration is now finished and we will restart all over. We do not want PyTorch to try to backpropagate the error back to the first iteration. \n",
    "\n",
    "To avoid that, we could manually restart the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1.grad.zero_()\n",
    "params2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, PyTorch optimizers can also do that automatically with a simple comand that we will examine in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use ``.detach()`` to detach a tensor from the computation history, e.g. to create a new tensor with the current value of another one but without its gradient history. This new tensor can be used for other purposes and further computation that is independent of how the original values were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"62pt\" height=\"39pt\"\n",
       " viewBox=\"0.00 0.00 62.00 39.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 35)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-35 58,-35 58,4 -4,4\"/>\n",
       "<!-- 139876765744152 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139876765744152</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"54,-31 0,-31 0,0 54,0 54,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\" fill=\"#000000\"> ()</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f381c706eb8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loss = loss.detach()\n",
    "torchviz.make_dot(new_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's package [torch.optim](https://pytorch.org/docs/stable/optim.html?highlight=optimizers) implements several common optimization methods such as SGD, Adam, AdaGrad, RMSprop etc. It lets us easily choose the learning rate, momentum and other hyperparameters, perform updating steps in our models with the method ``.step()`` and clean gradient history with the ``.zero_grad()`` method.\n",
    "\n",
    "Let us use the Adam optimizer to perform a gradient descent step in the loss function we computed above.\n",
    "\n",
    "We first choose an optimizer and pass a list of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = torch.tensor([[1,1,1], [2,2,2], [3,3,3]], device=my_device, requires_grad=True, dtype=torch.float)\n",
    "params2 = torch.tensor([0.5, 0.1, 0.9], device=my_device, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "# first instantiate an optim object with the model's parameters and the optimizer's specific hyperparameters\n",
    "optimizer = optim.Adam([params1, params2], lr=0.0001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build the loss again and call the ``.step()`` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([10, 10, 10], device=my_device, requires_grad=False, dtype=torch.float)\n",
    "\n",
    "aux1 = params1.matmul(data)\n",
    "aux2 = params2**0.5\n",
    "aux3 = aux2.dot(aux1)\n",
    "aux4 = aux3.cos() + aux2.log2()\n",
    "\n",
    "loss = aux4.sum()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9999, 0.9999, 0.9999],\n",
       "         [1.9999, 1.9999, 1.9999],\n",
       "         [2.9999, 2.9999, 2.9999]], device='cuda:0', requires_grad=True),\n",
       " tensor([0.4999, 0.0999, 0.8999], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1, params2 # check new parameters after one update in gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.0195, 2.0195, 2.0195],\n",
       "         [0.9032, 0.9032, 0.9032],\n",
       "         [2.7095, 2.7095, 2.7095]], device='cuda:0'),\n",
       " tensor([ 7.5013, 34.3085, 14.3490], device='cuda:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1.grad, params2.grad # check how the optimizer computed and stored the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must then call ``optimizer.zero_grad()`` to make sure that the gradient computation starts again in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]], device='cuda:0'),\n",
       " tensor([0., 0., 0.], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1.grad, params2.grad # check how the optimizer zeroed the previous gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a pseudo loss function so far. However, neural network models normally use more elaborate loss functions like mean squared error, cross entropy, hinge etc. \n",
    "\n",
    "The most usual loss functions are already implemented in PyTorch's [``.nn`` package](https://pytorch.org/docs/stable/nn.html) with their corresponding gradient computation: L1Loss, MSELoss, CrossEntropyLoss, CTCLoss, NLLLoss, PoissonNLLLoss, KLDivLoss, BCELoss, BCEWithLogitsLoss, MarginRankingLoss,  HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss and TripletMarginLoss.\n",
    "\n",
    "Here is an example using the cross entropy with a single, non-normalized output scores and a golden class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0001)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we first instantiate a loss object, eventually passing the arguments with its hyperparameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scores = torch.tensor([[-5, 21, 30]], dtype=torch.float)\n",
    "gold = torch.tensor([2])\n",
    "\n",
    "loss = criterion(scores, gold)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a simple model Ax=y, where x is our data and A are parameters, and suppose that we know the correct output Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand(3,3, device=my_device, requires_grad=True) # random initialization of our parameters\n",
    "x = torch.tensor([0.1, 0.2, 0.3], device=my_device, requires_grad=True)\n",
    "\n",
    "y = A.matmul(x)\n",
    "Y = torch.tensor([0.2, 0.3, 0.4]).to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first instantiate a loss function from a class in ``torch.nn``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the loss by passing the output of our model and the true answers to the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse(y, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we can continue the optimization as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()    \n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's ``torch.nn`` module provides implementations of many [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity): ELU, Hardshrink, Hardtanh, LeakyReLU, LogSigmoid, MultiheadAttention, PReLU, ReLU, ReLU6, RReLU, SELU, CELU, Sigmoid, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Threshold, Softmin, Softmax, Softmax2d, LogSoftMax, AdaptiveLogSoftmaxWithLoss.\n",
    "\n",
    "Let us see how to create a Softmax and a ReLU layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0179, 0.0066, 0.9756])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we must first instantiate a class, then use it with our tensor as argument\n",
    "# calling F.softmax(output, dim=-1) would also work as a function, instead of a class\n",
    "\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "example_output = torch.tensor([-1,-2,3], dtype=torch.float)\n",
    "softmax(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 3.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling F.relu(example_output) would also work as a function, instead of a class\n",
    "\n",
    "activation = nn.ReLU()\n",
    "activation(example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered some important building blocks for training neural networks, but we still need to explore the models themselves. Like the loss functions, the main types of layers that compose a neural network (for instance, linear, convolutional, pooling, recurrent, dropout and transformer layer) are also implemented in the [``.nn`` package](https://pytorch.org/docs/stable/nn.html) as classes.\n",
    "\n",
    "All neural network models derive from a base class called Module. Whenever we create a new model, we should inherit the base class and write its ``__init__(self)`` method (where we define all layers) and ``forward(self, input)`` method (where we define the forward pass that returns an output). It's also common to write the ``init_weights(self)`` method too.\n",
    "\n",
    "The ``.forward()`` method should not be explicitly called though. PyTorch is built to work like that, calling the model instance itself with the arguments demanded by the ``.forward()`` method will take care of everything that must happen in the background.\n",
    "\n",
    "Let us see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e?#ea0d\n",
    "\n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a simple linear regression, the class ManualLinearRegression defines two parameters, initializing them randomly. The forward method returns the result of ax+b. \n",
    "\n",
    "The class ``nn.Parameter()`` is a subclass of ``Tensor``, which is automatically included into the model's parameters when we instantiate a subclass of ``nn.Module``.\n",
    "\n",
    "When we use PyTorch's implementation, we do not need to define the parameters explicitly when we choose a built-in layer. The ``LayerLinearRegression`` below is another way to do the same thing as ``ManualLinearRegression()`` using PyTorch resources: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e?#ea0d\n",
    "\n",
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another more elaborate example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/pytorch/examples/blob/master/mnist/main.py, modified\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)   # probability of an element to be zero-ed = 0.25\n",
    "        self.dropout2 = nn.Dropout2d(0.5)    # probability of an element to be zero-ed = 0.5\n",
    "        self.fc1 = nn.Linear(9216, 128)      # input with 9216 dims, output with 128 dims\n",
    "        self.fc2 = nn.Linear(128, 10)        # input with 128 dims, output with 10 dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "# The use of torch.nn.functional is deprecated according to https://discuss.pytorch.org/t/torch-tanh-vs-torch-nn-functional-tanh/15897.\n",
    "# We keep it here because we can still find it on existing codes, however all activation functions are available\n",
    "# on the module torch.nn as classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classifying MNIST digits, they create a more elaborate class called Net, which inherits from the base class Module. This model uses some of the many implemented layers in ``torch.nn``.\n",
    "\n",
    "The model is initialized with two convolutional layers, two dropout layers and two linear layers. Its forward method defines the forward pass: the input goes through convolution, ReLu, convolution, pooling, dropout, linearization, ReLu, droupout, linearizarion and softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the ManualLinearRegression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.6794], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor([-0.9951], device='cuda:0', requires_grad=True) tensor([-3.3009], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# example input and correct output\n",
    "x = torch.tensor([4], device=my_device)\n",
    "y_gold = 5\n",
    "\n",
    "model = ManualLinearRegression().to(my_device) # the model must be in the same device as the data\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "# Check the values:\n",
    "print(model.a, model.b, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-16.6018], device='cuda:0'), tensor([-66.4072], device='cuda:0'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = (output - y_gold)**2\n",
    "loss.backward()\n",
    "model.a.grad, model.b.grad # gradients have been computed for our parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the LayerLinearRegression does the same thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9242]], device='cuda:0', requires_grad=True), Parameter containing:\n",
      "tensor([-0.3923], device='cuda:0', requires_grad=True)]\n",
      "tensor([3.3044], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([4], device=my_device, dtype=torch.float)\n",
    "y_gold = 5\n",
    "model2 = LayerLinearRegression().to(my_device) # the model must be in the same device as the data\n",
    "\n",
    "output2 = model2(x)\n",
    "print([*model2.parameters()])\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the values of the parameters in our model, we can use the generator ``.parameters()`` as above or call ``.state_dict()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('a', tensor([0.6794], device='cuda:0')),\n",
       "             ('b', tensor([-0.9951], device='cuda:0'))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight', tensor([[0.9242]], device='cuda:0')),\n",
       "             ('linear.bias', tensor([-0.3923], device='cuda:0'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a generator again, but with the corresponding names of the many parameters in the Net() model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1.weight', Parameter containing:\n",
       "  tensor([[[[-0.3284,  0.1299, -0.1249],\n",
       "            [-0.3066, -0.2156,  0.0285],\n",
       "            [-0.1348, -0.1612,  0.0749]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2943,  0.0075,  0.2639],\n",
       "            [ 0.2597, -0.0793,  0.0755],\n",
       "            [ 0.3102,  0.1647, -0.1358]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2238,  0.1071,  0.2442],\n",
       "            [-0.2883,  0.2338,  0.0058],\n",
       "            [ 0.1216, -0.2839,  0.0811]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2298,  0.2838, -0.1426],\n",
       "            [ 0.2503,  0.1872,  0.0005],\n",
       "            [ 0.0811, -0.0444,  0.1331]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2375,  0.2897, -0.0719],\n",
       "            [ 0.0394, -0.1484, -0.0700],\n",
       "            [ 0.1333,  0.0100,  0.1924]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1241, -0.1512, -0.0964],\n",
       "            [-0.0309, -0.3181,  0.0661],\n",
       "            [-0.0798, -0.0646, -0.0151]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1101, -0.2016, -0.0180],\n",
       "            [ 0.0331,  0.0271, -0.1313],\n",
       "            [-0.2177, -0.2281,  0.1123]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2493, -0.0552, -0.1958],\n",
       "            [-0.0989,  0.2365, -0.1178],\n",
       "            [ 0.3106, -0.2479, -0.1340]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2680, -0.2120,  0.0256],\n",
       "            [ 0.0502, -0.2981, -0.1543],\n",
       "            [-0.2377, -0.0739, -0.0770]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2400, -0.1070, -0.2974],\n",
       "            [-0.3330,  0.2424,  0.1726],\n",
       "            [-0.2479,  0.1803, -0.0992]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2980,  0.0329, -0.2286],\n",
       "            [-0.0389,  0.1838, -0.3124],\n",
       "            [ 0.1171,  0.1279,  0.3188]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.3142,  0.1351, -0.1729],\n",
       "            [-0.1085,  0.2100,  0.2839],\n",
       "            [-0.2721,  0.1641,  0.0210]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2563,  0.2579,  0.2691],\n",
       "            [-0.2072,  0.1075, -0.1451],\n",
       "            [-0.0977,  0.0026,  0.3292]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0516,  0.3214, -0.0471],\n",
       "            [ 0.2381, -0.0193,  0.1420],\n",
       "            [ 0.1701,  0.2278, -0.0100]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1525, -0.2577,  0.0298],\n",
       "            [ 0.0944, -0.0798, -0.2822],\n",
       "            [-0.1675, -0.0356, -0.2147]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2337, -0.2482, -0.2293],\n",
       "            [-0.0307, -0.3096,  0.2950],\n",
       "            [ 0.1610, -0.2034, -0.2125]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3198,  0.0703, -0.2542],\n",
       "            [-0.1007, -0.1224,  0.2855],\n",
       "            [-0.0483, -0.2230, -0.2615]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.3146, -0.1776,  0.2236],\n",
       "            [ 0.0757, -0.0180,  0.0910],\n",
       "            [ 0.1998, -0.2300, -0.3117]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.3079,  0.0672,  0.2337],\n",
       "            [ 0.1802, -0.1592, -0.1083],\n",
       "            [ 0.2423,  0.3275, -0.0053]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.2837,  0.1427, -0.2978],\n",
       "            [ 0.0917,  0.0932, -0.0100],\n",
       "            [-0.1623, -0.0213,  0.1352]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2342, -0.2772, -0.0257],\n",
       "            [-0.1575,  0.0879,  0.2184],\n",
       "            [-0.0387, -0.2701, -0.0430]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1868, -0.1488,  0.1515],\n",
       "            [-0.3323, -0.1737,  0.0160],\n",
       "            [-0.2021,  0.3261,  0.0933]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0991, -0.0886,  0.0674],\n",
       "            [-0.0083,  0.0238,  0.1278],\n",
       "            [ 0.2085, -0.0422,  0.1551]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1399,  0.1811, -0.2138],\n",
       "            [ 0.1095, -0.2967,  0.3217],\n",
       "            [ 0.2144,  0.2273,  0.0258]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0004,  0.0163, -0.0371],\n",
       "            [ 0.2328, -0.2381,  0.3326],\n",
       "            [ 0.3095, -0.3302,  0.0714]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1516, -0.0640,  0.2698],\n",
       "            [-0.0609, -0.1650,  0.0188],\n",
       "            [ 0.2870,  0.0286, -0.0337]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0066, -0.1202,  0.1514],\n",
       "            [ 0.2948,  0.2638,  0.0964],\n",
       "            [ 0.2144,  0.0071,  0.3085]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2248,  0.1656, -0.2617],\n",
       "            [-0.1474,  0.1392,  0.1388],\n",
       "            [ 0.0233, -0.2861,  0.0536]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1489, -0.0410,  0.2319],\n",
       "            [ 0.0170, -0.2254,  0.1188],\n",
       "            [-0.0140, -0.1779,  0.0750]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2481, -0.0769, -0.2136],\n",
       "            [-0.3151, -0.1014,  0.0935],\n",
       "            [-0.0353, -0.3231,  0.3064]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0805, -0.2280, -0.0011],\n",
       "            [-0.2533,  0.0355,  0.1339],\n",
       "            [ 0.2746,  0.1296, -0.2083]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0570,  0.0286,  0.1325],\n",
       "            [ 0.0723, -0.0934, -0.0499],\n",
       "            [ 0.1295, -0.0841, -0.3314]]]], requires_grad=True)),\n",
       " ('conv1.bias', Parameter containing:\n",
       "  tensor([-0.0287, -0.2887, -0.1737,  0.0167, -0.0992, -0.1926,  0.1225, -0.1894,\n",
       "          -0.1994, -0.3001, -0.2475,  0.1621, -0.0865, -0.1036,  0.2325, -0.1095,\n",
       "           0.0417, -0.2136,  0.2576,  0.1701,  0.2534, -0.1143,  0.0984,  0.1760,\n",
       "           0.2699,  0.1826,  0.1964,  0.0733, -0.1028,  0.1649,  0.1142, -0.1469],\n",
       "         requires_grad=True)),\n",
       " ('conv2.weight', Parameter containing:\n",
       "  tensor([[[[ 0.0373,  0.0204,  0.0490],\n",
       "            [ 0.0226, -0.0354, -0.0025],\n",
       "            [-0.0427, -0.0196,  0.0485]],\n",
       "  \n",
       "           [[-0.0293, -0.0024, -0.0581],\n",
       "            [-0.0267, -0.0518, -0.0516],\n",
       "            [ 0.0454, -0.0180, -0.0438]],\n",
       "  \n",
       "           [[ 0.0281, -0.0078, -0.0028],\n",
       "            [ 0.0375,  0.0415,  0.0418],\n",
       "            [ 0.0452,  0.0131, -0.0085]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0010, -0.0498, -0.0254],\n",
       "            [ 0.0524,  0.0026, -0.0340],\n",
       "            [ 0.0468, -0.0556,  0.0416]],\n",
       "  \n",
       "           [[ 0.0195,  0.0516,  0.0357],\n",
       "            [ 0.0216,  0.0229,  0.0418],\n",
       "            [ 0.0456,  0.0056, -0.0069]],\n",
       "  \n",
       "           [[-0.0249, -0.0563,  0.0536],\n",
       "            [-0.0025,  0.0470, -0.0405],\n",
       "            [-0.0359, -0.0174,  0.0289]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0119, -0.0503,  0.0034],\n",
       "            [-0.0059, -0.0028,  0.0268],\n",
       "            [ 0.0038,  0.0037, -0.0148]],\n",
       "  \n",
       "           [[-0.0335, -0.0527,  0.0039],\n",
       "            [-0.0491,  0.0533, -0.0310],\n",
       "            [ 0.0321,  0.0253, -0.0346]],\n",
       "  \n",
       "           [[-0.0272,  0.0531, -0.0277],\n",
       "            [-0.0524,  0.0301,  0.0173],\n",
       "            [ 0.0240, -0.0077, -0.0128]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0265, -0.0587, -0.0561],\n",
       "            [-0.0497, -0.0566, -0.0330],\n",
       "            [-0.0363, -0.0191, -0.0479]],\n",
       "  \n",
       "           [[ 0.0075,  0.0303, -0.0150],\n",
       "            [ 0.0050, -0.0395,  0.0130],\n",
       "            [-0.0582,  0.0357,  0.0369]],\n",
       "  \n",
       "           [[-0.0225,  0.0058, -0.0053],\n",
       "            [ 0.0317,  0.0294,  0.0383],\n",
       "            [ 0.0535, -0.0393,  0.0077]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0190, -0.0227, -0.0491],\n",
       "            [ 0.0346, -0.0159,  0.0047],\n",
       "            [-0.0104,  0.0186,  0.0499]],\n",
       "  \n",
       "           [[-0.0155, -0.0062,  0.0043],\n",
       "            [-0.0402, -0.0465,  0.0232],\n",
       "            [-0.0068,  0.0487,  0.0460]],\n",
       "  \n",
       "           [[ 0.0150, -0.0457, -0.0191],\n",
       "            [ 0.0301, -0.0507,  0.0111],\n",
       "            [-0.0248,  0.0334,  0.0353]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0404,  0.0445,  0.0369],\n",
       "            [ 0.0583,  0.0534, -0.0489],\n",
       "            [-0.0360, -0.0180, -0.0257]],\n",
       "  \n",
       "           [[ 0.0215,  0.0317,  0.0091],\n",
       "            [-0.0540, -0.0510,  0.0301],\n",
       "            [ 0.0436,  0.0538, -0.0396]],\n",
       "  \n",
       "           [[-0.0433,  0.0466, -0.0393],\n",
       "            [-0.0248,  0.0377,  0.0010],\n",
       "            [-0.0131, -0.0147,  0.0359]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[ 0.0587, -0.0548,  0.0203],\n",
       "            [ 0.0552, -0.0304,  0.0119],\n",
       "            [-0.0067, -0.0349,  0.0009]],\n",
       "  \n",
       "           [[-0.0062, -0.0502, -0.0569],\n",
       "            [-0.0191, -0.0267,  0.0049],\n",
       "            [ 0.0363, -0.0112, -0.0097]],\n",
       "  \n",
       "           [[ 0.0084,  0.0589, -0.0141],\n",
       "            [ 0.0490,  0.0585, -0.0268],\n",
       "            [ 0.0182,  0.0331, -0.0326]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0368,  0.0312,  0.0284],\n",
       "            [-0.0549,  0.0164, -0.0385],\n",
       "            [ 0.0172, -0.0318,  0.0015]],\n",
       "  \n",
       "           [[ 0.0218,  0.0282, -0.0039],\n",
       "            [ 0.0471,  0.0517, -0.0546],\n",
       "            [-0.0028, -0.0077,  0.0448]],\n",
       "  \n",
       "           [[-0.0327, -0.0095, -0.0281],\n",
       "            [-0.0058, -0.0065, -0.0113],\n",
       "            [ 0.0132,  0.0526,  0.0309]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0169,  0.0484, -0.0108],\n",
       "            [-0.0555,  0.0460,  0.0530],\n",
       "            [-0.0359, -0.0194,  0.0384]],\n",
       "  \n",
       "           [[-0.0251,  0.0308,  0.0194],\n",
       "            [ 0.0067, -0.0194, -0.0455],\n",
       "            [ 0.0091,  0.0345,  0.0315]],\n",
       "  \n",
       "           [[-0.0205,  0.0469, -0.0431],\n",
       "            [ 0.0490, -0.0424, -0.0388],\n",
       "            [ 0.0341, -0.0534,  0.0417]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.0555, -0.0004,  0.0310],\n",
       "            [-0.0070, -0.0424,  0.0576],\n",
       "            [ 0.0377, -0.0484,  0.0137]],\n",
       "  \n",
       "           [[ 0.0289,  0.0396,  0.0356],\n",
       "            [-0.0020, -0.0114,  0.0266],\n",
       "            [-0.0233, -0.0580,  0.0466]],\n",
       "  \n",
       "           [[-0.0146, -0.0339,  0.0356],\n",
       "            [-0.0002, -0.0056,  0.0153],\n",
       "            [-0.0319,  0.0078,  0.0072]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0352,  0.0471, -0.0529],\n",
       "            [-0.0285, -0.0280,  0.0327],\n",
       "            [ 0.0381,  0.0358, -0.0440]],\n",
       "  \n",
       "           [[ 0.0255,  0.0248, -0.0540],\n",
       "            [ 0.0127, -0.0105,  0.0183],\n",
       "            [ 0.0479, -0.0469, -0.0288]],\n",
       "  \n",
       "           [[ 0.0502, -0.0138,  0.0112],\n",
       "            [ 0.0079, -0.0257, -0.0192],\n",
       "            [-0.0068, -0.0511, -0.0303]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-0.0025,  0.0435, -0.0038],\n",
       "            [-0.0418, -0.0316, -0.0554],\n",
       "            [-0.0298, -0.0540, -0.0471]],\n",
       "  \n",
       "           [[-0.0046, -0.0188, -0.0131],\n",
       "            [ 0.0377,  0.0439,  0.0449],\n",
       "            [-0.0286, -0.0513,  0.0544]],\n",
       "  \n",
       "           [[-0.0456, -0.0314,  0.0362],\n",
       "            [ 0.0066, -0.0568,  0.0499],\n",
       "            [ 0.0258,  0.0118, -0.0312]]]], requires_grad=True)),\n",
       " ('conv2.bias', Parameter containing:\n",
       "  tensor([ 0.0227,  0.0131, -0.0460,  0.0208, -0.0521, -0.0195, -0.0050, -0.0014,\n",
       "           0.0158, -0.0169, -0.0212, -0.0031,  0.0381, -0.0274, -0.0586,  0.0326,\n",
       "          -0.0156, -0.0390, -0.0322,  0.0518, -0.0584, -0.0284,  0.0509,  0.0015,\n",
       "          -0.0116,  0.0006,  0.0026, -0.0292, -0.0108,  0.0136, -0.0144, -0.0042,\n",
       "          -0.0159,  0.0083, -0.0344,  0.0353,  0.0433,  0.0570,  0.0081, -0.0177,\n",
       "          -0.0268,  0.0288, -0.0336,  0.0172, -0.0018,  0.0418, -0.0569,  0.0115,\n",
       "           0.0140, -0.0277, -0.0105, -0.0063, -0.0279,  0.0162,  0.0131,  0.0477,\n",
       "          -0.0323,  0.0206,  0.0390, -0.0399,  0.0275,  0.0299,  0.0423, -0.0242],\n",
       "         requires_grad=True)),\n",
       " ('fc1.weight', Parameter containing:\n",
       "  tensor([[-0.0098,  0.0048, -0.0074,  ..., -0.0004, -0.0067, -0.0068],\n",
       "          [ 0.0075,  0.0044, -0.0070,  ..., -0.0031, -0.0075,  0.0094],\n",
       "          [ 0.0044, -0.0024,  0.0100,  ..., -0.0074, -0.0045, -0.0040],\n",
       "          ...,\n",
       "          [-0.0044, -0.0057, -0.0051,  ..., -0.0083, -0.0011,  0.0068],\n",
       "          [ 0.0050, -0.0005,  0.0066,  ..., -0.0035, -0.0057,  0.0037],\n",
       "          [-0.0044,  0.0018, -0.0038,  ...,  0.0062, -0.0093,  0.0042]],\n",
       "         requires_grad=True)),\n",
       " ('fc1.bias', Parameter containing:\n",
       "  tensor([-6.0702e-03,  4.0400e-03,  4.9327e-03,  2.5001e-03, -7.0486e-03,\n",
       "          -9.5103e-03, -9.3412e-03, -2.2608e-03, -3.8276e-03,  2.3121e-03,\n",
       "           2.0392e-03, -6.9788e-03, -7.8310e-03,  9.1701e-04, -6.7338e-03,\n",
       "           2.8880e-03, -8.8689e-03,  4.4032e-03, -1.0317e-02, -9.3601e-03,\n",
       "           9.8178e-03, -3.0761e-03,  8.9770e-04, -4.7442e-03, -9.0849e-04,\n",
       "           6.4450e-03,  4.5409e-03,  3.7832e-03, -3.4946e-03, -5.0887e-03,\n",
       "           7.5482e-04,  3.3971e-03, -5.2553e-03,  8.1145e-03,  6.8398e-03,\n",
       "           6.4238e-03,  9.0123e-03, -5.5126e-03,  7.8858e-03,  4.3851e-03,\n",
       "          -2.2290e-03,  4.8134e-03,  1.4629e-03, -4.3280e-03,  7.9304e-03,\n",
       "          -7.3906e-03,  8.9949e-03, -9.7076e-03, -6.6263e-03,  9.5139e-03,\n",
       "          -3.3698e-03, -4.1149e-03,  5.3386e-03,  8.1502e-03, -1.0011e-02,\n",
       "          -1.0281e-02, -5.9903e-03, -5.5786e-03,  8.4047e-03,  8.5872e-03,\n",
       "           8.5829e-03,  2.8078e-03, -5.4696e-03,  9.2648e-06, -4.5102e-03,\n",
       "          -2.1066e-03,  3.9294e-04, -1.0399e-02, -1.5023e-03,  7.1921e-03,\n",
       "           4.8540e-03, -3.4471e-03,  4.5487e-03, -1.9220e-03, -4.3022e-03,\n",
       "          -2.9079e-03,  4.0082e-03,  6.4819e-03,  3.0255e-04, -1.8293e-03,\n",
       "          -9.8262e-03,  6.1992e-03, -2.7317e-03, -1.7535e-03, -9.8465e-03,\n",
       "           6.4612e-03, -8.0581e-03,  1.1576e-03,  3.7469e-03, -2.7491e-04,\n",
       "          -1.1301e-03,  9.6833e-04,  4.7036e-03, -5.1989e-03, -1.7037e-03,\n",
       "           7.5696e-03,  7.5125e-03, -1.3735e-03, -4.4808e-03,  5.8857e-04,\n",
       "          -4.8647e-03, -2.7945e-03,  1.6129e-04,  3.5899e-03,  2.9663e-03,\n",
       "          -7.8288e-03, -4.8444e-03,  2.3041e-03,  9.7535e-03,  7.3550e-03,\n",
       "           8.0171e-03,  1.2408e-03, -2.7570e-03, -6.1280e-03, -8.9615e-03,\n",
       "          -7.6124e-03,  7.5434e-03, -2.1039e-03,  3.5944e-03, -7.0907e-03,\n",
       "           4.7167e-03,  3.8876e-03,  7.7710e-03, -7.1188e-04, -2.1672e-03,\n",
       "          -2.4239e-03, -1.0323e-02, -4.3206e-03], requires_grad=True)),\n",
       " ('fc2.weight', Parameter containing:\n",
       "  tensor([[-0.0371,  0.0583,  0.0051,  ..., -0.0751,  0.0381,  0.0375],\n",
       "          [ 0.0471,  0.0243,  0.0262,  ..., -0.0688, -0.0672,  0.0787],\n",
       "          [-0.0705, -0.0602, -0.0736,  ..., -0.0590,  0.0624,  0.0794],\n",
       "          ...,\n",
       "          [ 0.0497, -0.0208, -0.0514,  ...,  0.0483, -0.0042, -0.0712],\n",
       "          [-0.0123,  0.0858, -0.0368,  ...,  0.0241,  0.0744, -0.0751],\n",
       "          [-0.0804,  0.0400, -0.0714,  ...,  0.0359, -0.0069, -0.0358]],\n",
       "         requires_grad=True)),\n",
       " ('fc2.bias', Parameter containing:\n",
       "  tensor([-0.0639,  0.0045, -0.0305, -0.0680, -0.0789, -0.0099, -0.0152,  0.0668,\n",
       "          -0.0370, -0.0310], requires_grad=True))]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "[*net.named_parameters()] # named_parameters is a generator object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have many layers in a sequence, a handful shortcut is using ``nn.sequential`` to define your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/pytorch/examples/blob/master/mnist/main.py, modified\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "                        nn.Conv2d(1, 32, 3, 1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Conv2d(32, 64, 3, 1),\n",
    "                        nn.max_pool2d(x, 2),\n",
    "                        nn.Dropout2d(0.25),\n",
    "                        torch.flatten(x, 1),\n",
    "                        nn.Linear(9216, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout2d(0.5),    \n",
    "                        nn.Linear(128, 10),\n",
    "                        nn.log_softmax(x, dim=1)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "# The use of torch.nn.functional is deprecated according to https://discuss.pytorch.org/t/torch-tanh-vs-torch-nn-functional-tanh/15897.\n",
    "# We keep it here because we can still find it on existing codes, however all activation functions are available\n",
    "# on the module torch.nn as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([100, 1, 28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3339, -2.3586, -2.3455, -2.3623, -2.4616, -2.2878, -2.2460, -2.2004,\n",
      "         -2.2304, -2.2284],\n",
      "        [-2.2705, -2.2114, -2.2404, -2.4187, -2.5129, -2.3681, -2.3337, -2.1263,\n",
      "         -2.2733, -2.3241],\n",
      "        [-2.2576, -2.2621, -2.3070, -2.3989, -2.3797, -2.4093, -2.2059, -2.1772,\n",
      "         -2.3590, -2.2985],\n",
      "        [-2.3101, -2.2672, -2.3070, -2.4998, -2.5213, -2.3270, -2.2226, -2.1445,\n",
      "         -2.1948, -2.2955],\n",
      "        [-2.2584, -2.2919, -2.2782, -2.5003, -2.3621, -2.3951, -2.1992, -2.2281,\n",
      "         -2.3313, -2.2192],\n",
      "        [-2.2275, -2.3908, -2.3262, -2.3679, -2.5012, -2.3275, -2.2271, -2.2189,\n",
      "         -2.1915, -2.2881],\n",
      "        [-2.2561, -2.2539, -2.2910, -2.4088, -2.4099, -2.3459, -2.2361, -2.2717,\n",
      "         -2.3388, -2.2337],\n",
      "        [-2.1684, -2.3085, -2.3348, -2.3497, -2.4451, -2.3765, -2.2934, -2.1809,\n",
      "         -2.3125, -2.2874],\n",
      "        [-2.2527, -2.2464, -2.2634, -2.4588, -2.4640, -2.3011, -2.2204, -2.2743,\n",
      "         -2.2678, -2.3090],\n",
      "        [-2.2338, -2.2768, -2.3271, -2.4394, -2.4349, -2.3408, -2.2584, -2.2640,\n",
      "         -2.2614, -2.2167],\n",
      "        [-2.1762, -2.2527, -2.2771, -2.4194, -2.4572, -2.3562, -2.1817, -2.2744,\n",
      "         -2.3576, -2.3121],\n",
      "        [-2.3147, -2.3600, -2.2988, -2.3406, -2.3986, -2.2877, -2.2368, -2.1438,\n",
      "         -2.3817, -2.2889],\n",
      "        [-2.2898, -2.3544, -2.2397, -2.4130, -2.3472, -2.3362, -2.2451, -2.2628,\n",
      "         -2.3105, -2.2424],\n",
      "        [-2.2676, -2.3270, -2.2912, -2.3281, -2.4219, -2.3733, -2.2369, -2.2163,\n",
      "         -2.3239, -2.2575],\n",
      "        [-2.3071, -2.2548, -2.2264, -2.3545, -2.4532, -2.2499, -2.2588, -2.2658,\n",
      "         -2.3771, -2.3003],\n",
      "        [-2.2241, -2.3155, -2.2523, -2.4386, -2.4250, -2.3942, -2.2475, -2.1302,\n",
      "         -2.3335, -2.3074],\n",
      "        [-2.2869, -2.3102, -2.3856, -2.4025, -2.4458, -2.3222, -2.1605, -2.1970,\n",
      "         -2.2862, -2.2644],\n",
      "        [-2.3033, -2.3614, -2.2478, -2.4832, -2.3889, -2.2473, -2.3216, -2.2272,\n",
      "         -2.3079, -2.1730],\n",
      "        [-2.2362, -2.3647, -2.2293, -2.3421, -2.3828, -2.3369, -2.3643, -2.1729,\n",
      "         -2.3589, -2.2622],\n",
      "        [-2.2372, -2.3237, -2.2439, -2.4287, -2.3549, -2.3522, -2.2730, -2.1672,\n",
      "         -2.3833, -2.2894],\n",
      "        [-2.2839, -2.2928, -2.2345, -2.4355, -2.3457, -2.3024, -2.2714, -2.1303,\n",
      "         -2.3699, -2.3940],\n",
      "        [-2.1440, -2.2507, -2.2572, -2.4253, -2.4521, -2.3991, -2.2130, -2.2364,\n",
      "         -2.4669, -2.2403],\n",
      "        [-2.2779, -2.2953, -2.2504, -2.3855, -2.4675, -2.2712, -2.2896, -2.2162,\n",
      "         -2.3973, -2.2065],\n",
      "        [-2.2272, -2.2584, -2.2605, -2.4133, -2.4728, -2.4220, -2.2762, -2.1392,\n",
      "         -2.2820, -2.3203],\n",
      "        [-2.2537, -2.1589, -2.2340, -2.4947, -2.5324, -2.3757, -2.2979, -2.1277,\n",
      "         -2.2870, -2.3397],\n",
      "        [-2.1953, -2.3004, -2.2314, -2.3622, -2.3734, -2.4221, -2.2793, -2.2365,\n",
      "         -2.3659, -2.2837],\n",
      "        [-2.2327, -2.3192, -2.2743, -2.3478, -2.4373, -2.3994, -2.2948, -2.1463,\n",
      "         -2.3460, -2.2601],\n",
      "        [-2.2696, -2.2871, -2.3074, -2.4751, -2.4858, -2.3821, -2.1632, -2.2235,\n",
      "         -2.2175, -2.2660],\n",
      "        [-2.2459, -2.2537, -2.2582, -2.4674, -2.4761, -2.3600, -2.2477, -2.2170,\n",
      "         -2.3065, -2.2337],\n",
      "        [-2.2222, -2.3401, -2.2315, -2.3843, -2.4208, -2.3560, -2.2800, -2.1170,\n",
      "         -2.4262, -2.2917],\n",
      "        [-2.3018, -2.2721, -2.2947, -2.4750, -2.5418, -2.4002, -2.1702, -2.1658,\n",
      "         -2.2047, -2.2696],\n",
      "        [-2.3040, -2.3322, -2.2818, -2.3784, -2.3979, -2.2397, -2.2477, -2.1784,\n",
      "         -2.4074, -2.2834],\n",
      "        [-2.2648, -2.2422, -2.3735, -2.3590, -2.4806, -2.2867, -2.2015, -2.3428,\n",
      "         -2.2148, -2.2918],\n",
      "        [-2.1849, -2.2677, -2.3101, -2.4336, -2.4877, -2.4235, -2.1999, -2.1572,\n",
      "         -2.3584, -2.2610],\n",
      "        [-2.3408, -2.3211, -2.2728, -2.3078, -2.4247, -2.3344, -2.2920, -2.1915,\n",
      "         -2.2454, -2.3126],\n",
      "        [-2.2062, -2.2655, -2.3235, -2.3872, -2.4703, -2.3836, -2.2095, -2.2707,\n",
      "         -2.3037, -2.2384],\n",
      "        [-2.2890, -2.3500, -2.2269, -2.4184, -2.4198, -2.2724, -2.3163, -2.1581,\n",
      "         -2.3428, -2.2624],\n",
      "        [-2.2506, -2.2636, -2.3203, -2.4156, -2.4464, -2.4385, -2.1915, -2.1591,\n",
      "         -2.3200, -2.2654],\n",
      "        [-2.2379, -2.2739, -2.2481, -2.4362, -2.3970, -2.4783, -2.2045, -2.1668,\n",
      "         -2.3966, -2.2390],\n",
      "        [-2.3498, -2.2382, -2.2610, -2.4052, -2.4856, -2.2359, -2.2033, -2.2118,\n",
      "         -2.3470, -2.3258],\n",
      "        [-2.2654, -2.2648, -2.3139, -2.4139, -2.3968, -2.2941, -2.2764, -2.1668,\n",
      "         -2.3946, -2.2659],\n",
      "        [-2.2442, -2.3867, -2.2845, -2.3951, -2.4197, -2.3665, -2.2079, -2.2144,\n",
      "         -2.2959, -2.2398],\n",
      "        [-2.2722, -2.3520, -2.3757, -2.3077, -2.5213, -2.3989, -2.1624, -2.1829,\n",
      "         -2.3149, -2.1930],\n",
      "        [-2.3321, -2.3135, -2.2942, -2.3829, -2.4168, -2.3931, -2.1976, -2.1387,\n",
      "         -2.3705, -2.2256],\n",
      "        [-2.2558, -2.1117, -2.2359, -2.4611, -2.4892, -2.2979, -2.2568, -2.2538,\n",
      "         -2.3600, -2.3604],\n",
      "        [-2.2522, -2.2733, -2.3774, -2.4418, -2.4110, -2.3857, -2.2236, -2.1572,\n",
      "         -2.2910, -2.2511],\n",
      "        [-2.1751, -2.2940, -2.3098, -2.3503, -2.4446, -2.4306, -2.2212, -2.2030,\n",
      "         -2.3782, -2.2585],\n",
      "        [-2.2263, -2.3221, -2.3237, -2.3470, -2.4589, -2.3015, -2.2138, -2.2568,\n",
      "         -2.3086, -2.2884],\n",
      "        [-2.1737, -2.2405, -2.2532, -2.4141, -2.4749, -2.3813, -2.2099, -2.2430,\n",
      "         -2.4450, -2.2428],\n",
      "        [-2.3178, -2.3286, -2.2534, -2.3498, -2.4060, -2.2962, -2.2076, -2.2329,\n",
      "         -2.3581, -2.2922],\n",
      "        [-2.2040, -2.2416, -2.2668, -2.4623, -2.4427, -2.3848, -2.2092, -2.2236,\n",
      "         -2.3796, -2.2552],\n",
      "        [-2.2510, -2.3011, -2.3004, -2.3847, -2.4455, -2.3782, -2.2709, -2.2332,\n",
      "         -2.2668, -2.2185],\n",
      "        [-2.2417, -2.2280, -2.3039, -2.4527, -2.4205, -2.3404, -2.2623, -2.2473,\n",
      "         -2.3035, -2.2523],\n",
      "        [-2.2711, -2.2601, -2.2799, -2.4254, -2.4766, -2.3205, -2.2394, -2.2412,\n",
      "         -2.3227, -2.2200],\n",
      "        [-2.2791, -2.3499, -2.3041, -2.3333, -2.3726, -2.3248, -2.2817, -2.2573,\n",
      "         -2.3037, -2.2279],\n",
      "        [-2.1826, -2.2041, -2.3009, -2.4137, -2.4414, -2.4009, -2.3011, -2.1563,\n",
      "         -2.3659, -2.3044],\n",
      "        [-2.3068, -2.2928, -2.2781, -2.3943, -2.4454, -2.3227, -2.2912, -2.1057,\n",
      "         -2.3065, -2.3177],\n",
      "        [-2.2840, -2.3391, -2.2988, -2.3967, -2.4436, -2.3694, -2.2374, -2.1348,\n",
      "         -2.2497, -2.3076],\n",
      "        [-2.3086, -2.3162, -2.3233, -2.3701, -2.4708, -2.3656, -2.2235, -2.1441,\n",
      "         -2.2795, -2.2596],\n",
      "        [-2.1533, -2.2298, -2.3202, -2.3938, -2.5438, -2.4168, -2.1848, -2.2247,\n",
      "         -2.3538, -2.2689],\n",
      "        [-2.3142, -2.2941, -2.3010, -2.3924, -2.4958, -2.2998, -2.1953, -2.2000,\n",
      "         -2.2690, -2.2977],\n",
      "        [-2.3442, -2.3360, -2.3208, -2.3220, -2.3531, -2.3606, -2.2879, -2.1802,\n",
      "         -2.3336, -2.2053],\n",
      "        [-2.3197, -2.2925, -2.3582, -2.3467, -2.4699, -2.3407, -2.2406, -2.1145,\n",
      "         -2.2985, -2.2823],\n",
      "        [-2.1987, -2.3347, -2.2546, -2.4997, -2.4117, -2.2805, -2.2141, -2.2085,\n",
      "         -2.3612, -2.3040],\n",
      "        [-2.1905, -2.3151, -2.2919, -2.3522, -2.4438, -2.4372, -2.2173, -2.2074,\n",
      "         -2.3720, -2.2374],\n",
      "        [-2.2540, -2.2359, -2.2365, -2.4931, -2.5053, -2.2909, -2.2983, -2.1911,\n",
      "         -2.3353, -2.2367],\n",
      "        [-2.3048, -2.2347, -2.3043, -2.4874, -2.3724, -2.3376, -2.2647, -2.1987,\n",
      "         -2.3924, -2.1698],\n",
      "        [-2.2903, -2.2835, -2.3007, -2.4379, -2.4468, -2.3570, -2.1862, -2.1966,\n",
      "         -2.3273, -2.2353],\n",
      "        [-2.3506, -2.2085, -2.1934, -2.4030, -2.3883, -2.2577, -2.3344, -2.1875,\n",
      "         -2.4104, -2.3267],\n",
      "        [-2.2346, -2.2478, -2.2932, -2.4424, -2.3901, -2.2790, -2.1840, -2.3103,\n",
      "         -2.4170, -2.2592],\n",
      "        [-2.2248, -2.2943, -2.3315, -2.4904, -2.4389, -2.3722, -2.2932, -2.1938,\n",
      "         -2.2001, -2.2322],\n",
      "        [-2.3056, -2.3630, -2.3523, -2.3612, -2.4323, -2.3015, -2.2659, -2.1391,\n",
      "         -2.2675, -2.2665],\n",
      "        [-2.2495, -2.2434, -2.2304, -2.3744, -2.4082, -2.4308, -2.2877, -2.2890,\n",
      "         -2.3423, -2.1983],\n",
      "        [-2.2302, -2.2087, -2.2601, -2.3960, -2.5490, -2.3910, -2.2123, -2.2321,\n",
      "         -2.3844, -2.2196],\n",
      "        [-2.1707, -2.1926, -2.3142, -2.3605, -2.4571, -2.3276, -2.2223, -2.2674,\n",
      "         -2.4677, -2.2914],\n",
      "        [-2.2970, -2.2350, -2.2599, -2.4564, -2.4243, -2.2776, -2.2750, -2.2039,\n",
      "         -2.2889, -2.3359],\n",
      "        [-2.2362, -2.3651, -2.2754, -2.4220, -2.3236, -2.3779, -2.2661, -2.1851,\n",
      "         -2.4224, -2.1880],\n",
      "        [-2.3100, -2.3180, -2.2725, -2.4170, -2.4005, -2.3070, -2.2219, -2.1330,\n",
      "         -2.4260, -2.2584],\n",
      "        [-2.2777, -2.3000, -2.2739, -2.3418, -2.4110, -2.2752, -2.2542, -2.2467,\n",
      "         -2.4425, -2.2255],\n",
      "        [-2.3399, -2.3068, -2.2047, -2.5200, -2.3536, -2.2452, -2.2629, -2.1070,\n",
      "         -2.4101, -2.3334],\n",
      "        [-2.2831, -2.3543, -2.3219, -2.3096, -2.4011, -2.4047, -2.2578, -2.1208,\n",
      "         -2.3466, -2.2579],\n",
      "        [-2.2071, -2.2899, -2.3061, -2.4022, -2.4272, -2.3593, -2.2298, -2.2372,\n",
      "         -2.3936, -2.2059],\n",
      "        [-2.2363, -2.2765, -2.2754, -2.3908, -2.4554, -2.3965, -2.2292, -2.2371,\n",
      "         -2.2766, -2.2797],\n",
      "        [-2.2287, -2.2934, -2.3359, -2.3714, -2.4587, -2.4280, -2.1433, -2.1742,\n",
      "         -2.3687, -2.2731],\n",
      "        [-2.2613, -2.2785, -2.3194, -2.4763, -2.5111, -2.3363, -2.1536, -2.2569,\n",
      "         -2.2167, -2.2691],\n",
      "        [-2.2841, -2.2653, -2.2760, -2.3870, -2.3714, -2.3162, -2.2967, -2.1857,\n",
      "         -2.3814, -2.2795],\n",
      "        [-2.2839, -2.2629, -2.2937, -2.4096, -2.3749, -2.3159, -2.1681, -2.2246,\n",
      "         -2.4318, -2.2904],\n",
      "        [-2.2862, -2.2770, -2.3019, -2.4294, -2.5338, -2.3199, -2.2554, -2.1993,\n",
      "         -2.2625, -2.2059],\n",
      "        [-2.2203, -2.2812, -2.2844, -2.4432, -2.5163, -2.2863, -2.2707, -2.2739,\n",
      "         -2.2661, -2.2230],\n",
      "        [-2.2789, -2.3348, -2.2544, -2.3922, -2.4549, -2.2926, -2.2540, -2.1439,\n",
      "         -2.2886, -2.3650],\n",
      "        [-2.1983, -2.3335, -2.3018, -2.3347, -2.4294, -2.3650, -2.3107, -2.2153,\n",
      "         -2.3297, -2.2306],\n",
      "        [-2.2260, -2.2987, -2.2792, -2.3671, -2.3450, -2.3613, -2.2941, -2.2419,\n",
      "         -2.3803, -2.2466],\n",
      "        [-2.3325, -2.2899, -2.2800, -2.3546, -2.4164, -2.3093, -2.2861, -2.2165,\n",
      "         -2.2906, -2.2630],\n",
      "        [-2.3104, -2.2686, -2.3221, -2.4418, -2.4139, -2.2596, -2.2062, -2.1870,\n",
      "         -2.3352, -2.3104],\n",
      "        [-2.1895, -2.3161, -2.2206, -2.4212, -2.4165, -2.3450, -2.2648, -2.2808,\n",
      "         -2.3783, -2.2240],\n",
      "        [-2.3483, -2.2859, -2.2768, -2.3458, -2.4293, -2.2665, -2.2675, -2.2722,\n",
      "         -2.2551, -2.2915],\n",
      "        [-2.2674, -2.3130, -2.2574, -2.4275, -2.4278, -2.3450, -2.2373, -2.1605,\n",
      "         -2.3123, -2.3080],\n",
      "        [-2.2863, -2.2936, -2.3544, -2.4225, -2.4145, -2.3611, -2.2002, -2.1259,\n",
      "         -2.3529, -2.2549],\n",
      "        [-2.1962, -2.2403, -2.2824, -2.3858, -2.4264, -2.3734, -2.1489, -2.2840,\n",
      "         -2.4691, -2.2669],\n",
      "        [-2.3141, -2.2342, -2.2498, -2.4568, -2.4288, -2.2940, -2.2777, -2.1644,\n",
      "         -2.3742, -2.2681]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "outputs = net(x)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other tips "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html?highlight=reproducibility): we can give a seed value to the random number generator at the beginning of our code, so that we can reproduce the randomly initiliazed values of our model (e.g. the weights of a neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(22) # choose any integer\n",
    "torch.cuda.manual_seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Weight initialization](https://pytorch.org/docs/stable/nn.init.html?highlight=initialization): each layer implemented in PyTorch has a default weight initialization method. Still, we can choose one of the many initialization methods implemented on ``torch.nn.init``. After we create a layer, we can use them to initialize the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6550, 0.5360, 0.5379, 0.3995, 0.5657], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1 = nn.Linear(10, 5)\n",
    "torch.nn.init.uniform_(fc1.weight, a=0.0, b=1.0) # uniform random initializtion with numbers between 0 and 1\n",
    "torch.nn.init.uniform_(fc1.bias, a=0.0, b=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most layers include a bias term as default. We can change that by setting the argument ``bias`` to False when we instantiate a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.0105,  0.6447,  0.7000, -0.6737,  0.5332],\n",
      "        [ 0.5382,  0.5996, -0.1174, -0.3737,  0.3098],\n",
      "        [-0.2072,  0.5853, -0.5919,  0.1770, -0.1372],\n",
      "        [ 0.3013, -0.5669, -0.2654,  0.1224,  0.3774],\n",
      "        [-0.5567,  0.5766,  0.1534, -0.4828, -0.0714],\n",
      "        [-0.5483,  0.3734,  0.2936, -0.1256,  0.4017],\n",
      "        [ 0.0801, -0.1223, -0.6987, -0.2561, -0.5731],\n",
      "        [ 0.5322,  0.5665, -0.3863, -0.2940, -0.2376]], requires_grad=True)) ('weight_hh_l0', Parameter containing:\n",
      "tensor([[-0.6883, -0.1375],\n",
      "        [-0.0822, -0.5835],\n",
      "        [ 0.6837, -0.5780],\n",
      "        [-0.1257,  0.2416],\n",
      "        [ 0.5828, -0.5169],\n",
      "        [-0.1703, -0.1890],\n",
      "        [-0.2718,  0.1064],\n",
      "        [-0.5787, -0.2178]], requires_grad=True)) ('bias_ih_l0', Parameter containing:\n",
      "tensor([ 0.5380,  0.4018,  0.6273, -0.1422, -0.0426,  0.2606,  0.2750, -0.0779],\n",
      "       requires_grad=True)) ('bias_hh_l0', Parameter containing:\n",
      "tensor([ 0.1559,  0.0214, -0.0154, -0.3440,  0.4188, -0.6215, -0.3229,  0.0461],\n",
      "       requires_grad=True)) \n",
      "\n",
      "('weight_ih_l0', Parameter containing:\n",
      "tensor([[ 0.4618,  0.4109,  0.2484, -0.4759,  0.3639],\n",
      "        [ 0.4243,  0.4887,  0.5540, -0.4139,  0.5462],\n",
      "        [ 0.4052,  0.5749,  0.3734,  0.5403,  0.4751],\n",
      "        [ 0.5642, -0.2569,  0.5644, -0.4092, -0.6126],\n",
      "        [ 0.5798,  0.3133,  0.3942, -0.4583,  0.4546],\n",
      "        [-0.6630, -0.3104, -0.1055, -0.2924,  0.4040],\n",
      "        [ 0.6425,  0.3386, -0.2043, -0.0364, -0.2829],\n",
      "        [-0.2236, -0.2447,  0.5670, -0.1981,  0.1754]], requires_grad=True)) ('weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0278, -0.2352],\n",
      "        [-0.3616, -0.4865],\n",
      "        [ 0.2983,  0.3720],\n",
      "        [-0.5357,  0.2457],\n",
      "        [-0.5350, -0.3464],\n",
      "        [-0.1932,  0.1943],\n",
      "        [ 0.6191,  0.5068],\n",
      "        [-0.2027,  0.2891]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "lstm1 = nn.LSTM(5, 2)\n",
    "lstm2 = nn.LSTM(5, 2, bias=False)\n",
    "\n",
    "print(*lstm1.named_parameters(), '\\n')\n",
    "print(*lstm2.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are not training our model, we can envelope a code snippet under ``with torch.no_grad():``. Whithin this command, PyTorch will not track the gradient, which can be more efficient (less memory consumption) when we know we will not need to compute it (e.g. during testing/inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During evaluation, we can call ``model.eval()`` to enable the evaluation mode, which results in a different behavior that some layers should have when we are not training a model. For example, droupout normally happens only during training, but not during testing. This command thus avoids having to check all layers manually or building a separate model. During training, we must call ``model.train()`` to set it back to training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use gradient clipping, we can call ``torch.nn.utils.clip_grad_norm_(parameters, max_norm)`` after backpropagation but before updating the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods and functions that end with an _ perform in-place modifications. Both options are normally available for the usual operations on tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5])\n",
      "tensor([2, 3])\n",
      "tensor([5, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2])\n",
    "b = torch.tensor([2,3])\n",
    "\n",
    "a.add_(3) # in-place modification, a changes\n",
    "b.add(3) # b does not change, could be printed or stored in another variable\n",
    "c = b.add(3)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``torch.save()`` can be used to save a model to the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important***: New versions of Pytorch are released now and then, so parts of this tutorial as well as the tutorials you find online may not be up-to-date. Make sure to check the documentation of the version you are using whenever you implement a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further important commands, also check PyTorch's [cheat sheet](https://pytorch.org/tutorials/beginner/ptcheat.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
